{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas with Machine Learning\n",
    "### **Note Before You Start**\n",
    "\n",
    "Before diving into this notebook, it is important to have a basic understanding of machine learning concepts. Below are some fundamental topics that will help you follow along more effectively:\n",
    "\n",
    "1. **Supervised vs. Unsupervised Learning**: \n",
    "   - Understand the difference between supervised learning (where we have labeled data) and unsupervised learning (where we don't have labels).\n",
    "\n",
    "2. **Data Splitting (Train/Test Sets)**: \n",
    "   - Know why we split data into training and test sets, and the concept of model validation.\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Learn how to select, create, and transform features that will help improve your model's performance.\n",
    "\n",
    "4. **Data Preprocessing**:\n",
    "   - Concepts such as handling missing values, encoding categorical variables, and scaling numerical features are essential steps in preparing your data for machine learning.\n",
    "\n",
    "5. **Overfitting and Underfitting**:\n",
    "   - Understand the concepts of overfitting (when a model learns noise in the data) and underfitting (when a model is too simple to capture the underlying pattern).\n",
    "\n",
    "6. **Imbalanced Datasets**:\n",
    "   - Learn how imbalanced datasets can affect model performance and how to handle them with techniques like oversampling, undersampling, or using balanced metrics.\n",
    "\n",
    "7. **Model Evaluation Metrics**:\n",
    "   - Familiarize yourself with different metrics for evaluating model performance, such as accuracy, precision, recall, F1-score, and ROC-AUC for classification, and MSE or R-squared for regression.\n",
    "\n",
    "8. **Scikit-learn**:\n",
    "   - Have some familiarity with the `scikit-learn` library, as it will be used extensively for machine learning tasks like preprocessing, model training, and evaluation.\n",
    "\n",
    "Having a good grasp of these topics will help you better understand the code and the concepts explained throughout this notebook. If you are new to any of these topics, consider reviewing some foundational machine learning resources before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing Data for Machine Learning\n",
    "Before feeding data into machine learning algorithms, it's important to clean and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age    Salary\n",
      "4  38.75   80000.0\n",
      "2  45.00   70000.0\n",
      "0  25.00   50000.0\n",
      "3  50.00  120000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SP23-RAI-014.CUI\\AppData\\Local\\Temp\\ipykernel_12824\\592474600.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_ml['Age'].fillna(df_ml['Age'].mean(), inplace=True)\n",
      "C:\\Users\\SP23-RAI-014.CUI\\AppData\\Local\\Temp\\ipykernel_12824\\592474600.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_ml['Salary'].fillna(df_ml['Salary'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "df_ml = pd.DataFrame({\n",
    "    'Age': [25, 35, 45, 50, None],\n",
    "    'Salary': [50000, 60000, None, 120000, 80000],\n",
    "    'Purchased': ['No', 'Yes', 'No', 'Yes', 'No']\n",
    "})\n",
    "\n",
    "# Handling missing values\n",
    "df_ml['Age'].fillna(df_ml['Age'].mean(), inplace=True)\n",
    "df_ml['Salary'].fillna(df_ml['Salary'].median(), inplace=True)\n",
    "\n",
    "# Encoding categorical variables\n",
    "df_ml['Purchased'] = df_ml['Purchased'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X = df_ml[['Age', 'Salary']]\n",
    "y = df_ml['Purchased']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering with Pandas\n",
    "Creating new features or transforming existing features can help improve the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          City       Date  Sales  DayOfWeek  Sales_Ratio\n",
      "0     New York 2023-01-01    200     Sunday     0.161290\n",
      "1  Los Angeles 2023-01-02    300     Monday     0.241935\n",
      "2      Chicago 2023-01-03    250    Tuesday     0.201613\n",
      "3     New York 2023-01-04    220  Wednesday     0.177419\n",
      "4      Chicago 2023-01-05    270   Thursday     0.217742\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "df_engineering = pd.DataFrame({\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Chicago'],\n",
    "    'Date': pd.to_datetime(['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05']),\n",
    "    'Sales': [200, 300, 250, 220, 270]\n",
    "})\n",
    "\n",
    "# Creating new features from existing columns\n",
    "df_engineering['DayOfWeek'] = df_engineering['Date'].dt.day_name()\n",
    "df_engineering['Sales_Ratio'] = df_engineering['Sales'] / df_engineering['Sales'].sum()\n",
    "\n",
    "print(df_engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Integrating Pandas with Scikit-learn (train_test_split(), get_dummies())\n",
    "Pandas works seamlessly with Scikit-learn for encoding categorical variables and splitting data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Encoding categorical variables using get_dummies\n",
    "df_ml_dummies = pd.get_dummies(df_engineering, columns=['City', 'DayOfWeek'], drop_first=True)\n",
    "\n",
    "# Splitting data for Scikit-learn\n",
    "X = df_ml_dummies.drop(['Sales', 'Date'], axis=1)\n",
    "y = df_ml_dummies['Sales']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a simple model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training score:\", model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Handling Imbalanced Data\n",
    "Imbalanced datasets can lead to biased model predictions. Techniques like oversampling, undersampling, and SMOTE help balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    4\n",
      "1    4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Sample imbalanced dataset\n",
    "df_imbalanced = pd.DataFrame({\n",
    "    'Feature': [1, 2, 3, 4, 5, 6],\n",
    "    'Class': [0, 0, 0, 0, 1, 1]\n",
    "})\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority = df_imbalanced[df_imbalanced['Class'] == 1]\n",
    "df_majority = df_imbalanced[df_imbalanced['Class'] == 0]\n",
    "\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
    "\n",
    "# Combine the majority class with the upsampled minority class\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "print(df_balanced['Class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Scaling and Normalization\n",
    "Standardizing or normalizing features can help improve the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Income  Age_Standardized  Income_Normalized\n",
      "0   20   20000         -1.341641              0.000\n",
      "1   30   50000         -0.447214              0.375\n",
      "2   40   80000          0.447214              0.750\n",
      "3   50  100000          1.341641              1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Sample data for scaling\n",
    "df_scaling = pd.DataFrame({\n",
    "    'Age': [20, 30, 40, 50],\n",
    "    'Income': [20000, 50000, 80000, 100000]\n",
    "})\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "df_scaling['Age_Standardized'] = scaler.fit_transform(df_scaling[['Age']])\n",
    "\n",
    "# Normalization\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_scaling['Income_Normalized'] = minmax_scaler.fit_transform(df_scaling[['Income']])\n",
    "\n",
    "print(df_scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Working with Text Data for NLP\n",
    "Text data needs to be cleaned, tokenized, and vectorized before it can be used for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   analysis  data  for  fun  great  is  learning  machine  pandas  sentence  \\\n",
      "0         0     0    0    0      0   1         0        0       0         1   \n",
      "1         0     0    0    1      0   1         1        1       0         0   \n",
      "2         1     1    1    0      1   1         0        0       1         0   \n",
      "\n",
      "   this  \n",
      "0     1  \n",
      "1     0  \n",
      "2     0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "df_text = pd.DataFrame({\n",
    "    'Text': ['This is a sentence.', 'Machine learning is fun!', 'Pandas is great for data analysis.']\n",
    "})\n",
    "\n",
    "# Using CountVectorizer to transform text data into feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_text = vectorizer.fit_transform(df_text['Text'])\n",
    "\n",
    "# Display the feature vectors as a DataFrame\n",
    "df_vectorized = pd.DataFrame(X_text.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(df_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Feature Selection and Dimensionality Reduction\n",
    "Reducing the number of features can help improve model performance and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.26031933 0.23235218]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Generating synthetic data\n",
    "df_pca = pd.DataFrame(np.random.randn(100, 5), columns=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Feature5'])\n",
    "\n",
    "# Applying PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "df_reduced = pca.fit_transform(df_pca)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Data Transformation Pipelines\n",
    "Using pipelines ensures that all preprocessing steps are applied consistently during model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample data\n",
    "df_pipeline = pd.DataFrame({\n",
    "    'Feature1': [10, 20, 30, 40],\n",
    "    'Feature2': [100, 200, 300, 400]\n",
    "})\n",
    "y_pipeline = [0, 1, 0, 1]\n",
    "\n",
    "# Creating a pipeline for scaling and model training\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Training the pipeline\n",
    "pipeline.fit(df_pipeline, y_pipeline)\n",
    "\n",
    "# Predicting using the pipeline\n",
    "predictions = pipeline.predict(df_pipeline)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
