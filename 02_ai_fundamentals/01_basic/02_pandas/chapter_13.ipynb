{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Big Data (Beyond Pandas)\n",
    "### 1. Introduction to Dask \n",
    "Dask and Vaex are Python libraries designed to handle large datasets that donâ€™t fit into memory.\n",
    "- **Dask**: Parallelizes Pandas operations to work with large datasets in chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Dask\n",
    "# !pip install dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value of A: 0.0024709422254980775\n",
      "                  A         C         D         E\n",
      "B                                                \n",
      "-4.234288 -0.322151 -0.801109  0.927263 -1.061999\n",
      "-4.132184  0.212574  0.317399  0.433281  0.312023\n",
      "-4.029719  0.460794  0.783723 -1.180558  0.966778\n",
      "-4.013216  1.567691  0.346469 -0.272435  1.533439\n",
      "-3.916728  0.786227  2.086162 -1.018216 -0.343104\n"
     ]
    }
   ],
   "source": [
    "# Load a large CSV file using Dask\n",
    "dask_df = dd.read_csv('data/local/large_file.csv')\n",
    "\n",
    "# Perform basic operations like computing the mean of a column\n",
    "mean_value = dask_df['A'].mean().compute()\n",
    "print(\"Mean value of A:\", mean_value)\n",
    "\n",
    "# Group by a column and calculate the sum\n",
    "grouped_sum = dask_df.groupby('B').sum().compute()\n",
    "print(grouped_sum.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Handling Big Data in Pandas\n",
    "Pandas can handle relatively large datasets, but it requires optimization and careful memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B         C         D         E\n",
      "0 -0.722498  0.538341 -0.095738  1.398888  0.062597\n",
      "1  0.770041 -0.102221  1.012833  0.002186 -0.727072\n",
      "2 -1.999852  1.081321  0.544163 -0.058429 -0.349761\n",
      "3 -0.112298 -0.921257 -1.053475 -0.882685 -1.336251\n",
      "4 -0.845955  0.351747 -0.728274 -0.562573 -1.311634\n",
      "              A         B         C         D         E\n",
      "10000  0.752074 -0.802763  1.104975 -1.570371 -0.212752\n",
      "10001 -1.702163  0.368405 -0.657176 -0.083670 -0.150433\n",
      "10002  0.306042  0.965945  0.090297  1.357273 -0.145185\n",
      "10003  0.114770 -1.058163  0.426878 -0.943990  1.148841\n",
      "10004  1.125774  0.601294 -0.924858 -0.046938  0.630849\n",
      "              A         B         C         D         E\n",
      "20000 -0.676919  0.460245 -2.231921  0.553191  0.256725\n",
      "20001  0.958073  1.094360  1.271988 -1.359855 -1.702002\n",
      "20002 -0.030398  0.721667  0.017222  0.678820 -1.562772\n",
      "20003  1.233221  0.507481  0.710395  1.186792  1.398495\n",
      "20004 -2.054455 -0.220579 -0.219640 -1.205752 -0.214547\n",
      "              A         B         C         D         E\n",
      "30000 -1.056872 -0.927621 -3.365560 -1.283336  0.587232\n",
      "30001 -0.103375 -0.841312 -0.258219  1.675823 -0.321950\n",
      "30002 -1.115493 -2.603422 -0.732191  0.539205  0.868832\n",
      "30003  0.570716  0.367262 -0.276345 -1.303866  0.572504\n",
      "30004 -1.655066  0.889609 -0.113231 -1.658168 -0.262781\n",
      "              A         B         C         D         E\n",
      "40000  0.457738 -1.858737  1.298233  0.475711  0.624194\n",
      "40001  0.228607  1.360348 -1.977289  0.084835 -2.532033\n",
      "40002  0.166879  0.337521  0.181662  0.506741 -0.333264\n",
      "40003  0.313889  0.166129 -0.224240 -1.817666  0.432671\n",
      "40004  2.578031  0.231548  0.595914 -0.682931  0.090234\n",
      "              A         B         C         D         E\n",
      "50000 -1.202490  0.425620 -0.497545  0.651716 -3.196837\n",
      "50001  1.906515  0.675788 -1.057934 -0.062215  0.396490\n",
      "50002 -0.031274 -1.458502  0.574725 -0.662179  0.952841\n",
      "50003  0.092295  0.281097 -0.112468  0.954381  0.591625\n",
      "50004 -0.223266 -0.206295 -1.077240 -0.611628 -0.303468\n",
      "              A         B         C         D         E\n",
      "60000 -0.627103 -0.362100  1.088399 -0.946004  0.825941\n",
      "60001  0.080668 -1.616358  1.603554 -0.330923  0.065435\n",
      "60002  2.066716  0.550926  1.382767  0.684812 -1.022639\n",
      "60003 -0.378067 -1.208834 -1.315051 -0.865982  1.032401\n",
      "60004 -0.366938 -0.715980 -0.438077 -1.415933 -1.293074\n",
      "              A         B         C         D         E\n",
      "70000  0.541732 -0.972174 -0.559216  1.085184 -0.985440\n",
      "70001 -0.812469  0.984190 -0.367553  1.125739 -1.306153\n",
      "70002  0.907607 -0.042305  0.344658  0.186657  0.042985\n",
      "70003 -0.450978 -0.200173 -0.133995 -0.965147  1.064381\n",
      "70004 -1.010542 -0.481850  0.386590 -0.133651  1.701922\n",
      "              A         B         C         D         E\n",
      "80000  0.244615  0.029862  0.968331  0.536014 -0.202492\n",
      "80001 -1.199441 -0.523133  0.404606  0.284323  0.321246\n",
      "80002 -0.074021  1.664603  0.488141 -0.086819 -0.162046\n",
      "80003 -0.020547  0.867472  0.749607  0.951911  0.164652\n",
      "80004  0.130664  0.137865  0.945629 -0.179706  0.201399\n",
      "              A         B         C         D         E\n",
      "90000  3.829620  0.067324  0.898569  1.196524 -1.891669\n",
      "90001  0.258086  0.340741 -0.554639  0.130108  1.530096\n",
      "90002  0.100410  0.856401  0.630374 -0.684092 -0.245779\n",
      "90003 -0.219867  0.356580 -0.329888 -0.536505  0.352116\n",
      "90004  2.081534 -1.330209  0.795540 -0.842437 -1.187336\n"
     ]
    }
   ],
   "source": [
    "# Sample CSV loading with memory optimization\n",
    "types_dict = {\n",
    "    'column1': 'int32',\n",
    "    'column2': 'float32',\n",
    "    'column3': 'category'\n",
    "}\n",
    "\n",
    "# Load data in chunks and process each chunk\n",
    "chunk_size = 10000\n",
    "chunks = pd.read_csv('data/local/large_file.csv', dtype=types_dict, chunksize=chunk_size)\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Process each chunk\n",
    "    print(chunk.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scaling Pandas Operations on Large Datasets\n",
    "Scaling operations on large datasets with Dask to parallelize and optimize computations across multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas processing time: 0.1980609893798828 seconds\n",
      "Dask processing time: 0.1929333209991455 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Load a large dataset using Dask\n",
    "dask_large_df = dd.read_csv('data/local/large_file.csv')\n",
    "\n",
    "# Measuring time for an operation with Pandas\n",
    "start_time = time.time()\n",
    "pandas_df = pd.read_csv('data/local/large_file.csv')\n",
    "pandas_result = pandas_df.groupby('A').sum()\n",
    "print(f\"Pandas processing time: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Measuring time for an operation with Dask\n",
    "start_time = time.time()\n",
    "dask_result = dask_large_df.groupby('A').sum().compute()\n",
    "print(f\"Dask processing time: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Combining Pandas with Apache Spark for Big Data\n",
    "Apache Spark is a distributed data processing engine that can scale to large datasets efficiently, and Pandas can be used to integrate and analyze smaller chunks of Spark data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: double (nullable = true)\n",
      " |-- B: double (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      " |-- E: double (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|                   A|                   B|                   C|                   D|                  E|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "| -0.7224981161400361|  0.5383413302411633|-0.09573776363361086|  1.3988884728158737| 0.0625965156347914|\n",
      "|  0.7700405276674138|-0.10222058901775122|  1.0128334444560585|0.002185919898783305|-0.7270721625710608|\n",
      "|  -1.999851512549912|  1.0813214913952787|  0.5441629289221556|-0.05842924126490761|-0.3497607171684442|\n",
      "|-0.11229824892776323| -0.9212574430191187| -1.0534751072428974| -0.8826854237694995|-1.3362511503775372|\n",
      "| -0.8459546251440426|  0.3517471630058665| -0.7282737309144859| -0.5625731127756347|-1.3116336712252048|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "                 A            B            C            D            E\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000\n",
      "mean     -0.027152    -0.030426    -0.041149     0.010833     0.029645\n",
      "std       1.022960     1.030118     1.012574     0.987892     0.955223\n",
      "min      -2.790353    -3.260221    -3.572367    -3.117260    -3.132938\n",
      "25%      -0.725499    -0.716913    -0.743356    -0.607463    -0.641905\n",
      "50%      -0.016649    -0.013588    -0.057671     0.030419     0.042500\n",
      "75%       0.666438     0.687545     0.624894     0.631745     0.686667\n",
      "max       2.998391     3.014900     4.021412     3.249519     2.707981\n"
     ]
    }
   ],
   "source": [
    "# Importing PySpark (Ensure you have installed pyspark with java)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName('BigData').getOrCreate()\n",
    "\n",
    "# Load CSV into a Spark DataFrame\n",
    "spark_df = spark.read.csv('data/local/large_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema and first few rows\n",
    "spark_df.printSchema()\n",
    "spark_df.show(5)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame (for smaller data sizes)\n",
    "pandas_df_from_spark = spark_df.limit(1000).toPandas()\n",
    "\n",
    "# Performing some Pandas operations on the smaller dataset\n",
    "print(pandas_df_from_spark.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Case Studies on Big Data Handling with Pandas\n",
    "In this section, we'll explore how to handle large datasets in real-world scenarios using combinations of Pandas, Dask, and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Log Data Analysis\n",
    "#### Objective\n",
    "Analyze server log data to identify usage patterns, errors, and optimize server performance.\n",
    "\n",
    "#### Steps:\n",
    "1. **Load Large Dataset Efficiently**: Use Dask to load and preprocess large log data (e.g., in CSV or Parquet format).\n",
    "2. **Data Cleansing**: Handle missing values and filter logs based on criteria like status code, timestamp, or user ID.\n",
    "3. **Aggregation and Summarization**: Aggregate data to find frequent paths, error occurrences, and peak times.\n",
    "4. **Optimization**: Use Vaex to perform fast, memory-efficient operations on the cleaned data.\n",
    "5. **Reporting**: Sample a subset of the data into Pandas for visualization and final reporting.\n",
    "\n",
    "#### Tools & Libraries:\n",
    "- **Dask** for data loading and processing.\n",
    "- **Pandas** for detailed analysis and visualization.\n",
    "- **Vaex** for efficient data manipulation and transformation.\n",
    "\n",
    "By combining these tools, you can seamlessly handle large datasets, perform analyses, and generate insights from data that wouldn't fit in memory if processed with Pandas alone.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
