{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization\n",
    "### 1. Optimizing Memory Usage (dtype selection, category type)\n",
    "Choosing appropriate data types and converting columns to category type can save memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage:\n",
      "Index       132\n",
      "id        80000\n",
      "name     539602\n",
      "age       40000\n",
      "dtype: int64\n",
      "\n",
      "Optimized memory usage:\n",
      "Index      132\n",
      "id       80000\n",
      "name     10270\n",
      "age      40000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame\n",
    "df_memory = pd.DataFrame({\n",
    "    'id': range(1, 10001),\n",
    "    'name': np.random.choice(['Alice', 'Bob', 'Charlie'], size=10000),\n",
    "    'age': np.random.randint(18, 65, size=10000)\n",
    "})\n",
    "\n",
    "# Checking initial memory usage\n",
    "print(\"Initial memory usage:\")\n",
    "print(df_memory.memory_usage(deep=True))\n",
    "\n",
    "# Optimizing memory by using categorical dtype\n",
    "df_memory['name'] = df_memory['name'].astype('category')\n",
    "\n",
    "print(\"\\nOptimized memory usage:\")\n",
    "print(df_memory.memory_usage(deep=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vectorization vs. Loops\n",
    "Vectorization allows applying operations over entire arrays, leading to performance gains over traditional loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop execution time: 0.007524013519287109 seconds\n",
      "Vectorized execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Using a loop to add 10 to each value\n",
    "df_vector = pd.DataFrame({'values': range(1, 10001)})\n",
    "\n",
    "# Loop-based approach\n",
    "start_time = time.time()\n",
    "df_vector['values_plus_10_loop'] = [x + 10 for x in df_vector['values']]\n",
    "end_time = time.time()\n",
    "print(f\"Loop execution time: {end_time - start_time} seconds\")\n",
    "\n",
    "# Using vectorization to achieve the same result\n",
    "start_time = time.time()\n",
    "df_vector['values_plus_10_vector'] = df_vector['values'] + 10\n",
    "end_time = time.time()\n",
    "print(f\"Vectorized execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Efficient Iteration with iterrows() vs. itertuples()\n",
    "`itertuples()` is generally faster than `iterrows()` when iterating over rows in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterrows execution time: 0.0274050235748291 seconds\n",
      "Itetuples execution time: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame\n",
    "df_iteration = pd.DataFrame({\n",
    "    'A': np.random.randint(1, 100, size=1000),\n",
    "    'B': np.random.randint(1, 100, size=1000)\n",
    "})\n",
    "\n",
    "# Iteration using iterrows()\n",
    "start_time = time.time()\n",
    "for index, row in df_iteration.iterrows():\n",
    "    _ = row['A'] + row['B']\n",
    "end_time = time.time()\n",
    "print(f\"Iterrows execution time: {end_time - start_time} seconds\")\n",
    "\n",
    "# Iteration using itertuples()\n",
    "start_time = time.time()\n",
    "for row in df_iteration.itertuples():\n",
    "    _ = row.A + row.B\n",
    "end_time = time.time()\n",
    "print(f\"Itetuples execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parallel Processing with Pandas\n",
    "Using parallel processing to speed up data operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>A_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>3969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>3600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A  A_squared\n",
       "0  33       1089\n",
       "1  63       3969\n",
       "2  57       3249\n",
       "3  60       3600\n",
       "4  43       1849"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "\n",
    "# Initializing pandarallel\n",
    "pandarallel.initialize()\n",
    "\n",
    "# Sample DataFrame for parallel processing\n",
    "df_parallel = pd.DataFrame({\n",
    "    'A': np.random.randint(1, 100, size=10000)\n",
    "})\n",
    "\n",
    "# Function to apply\n",
    "def compute_square(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Using parallel apply\n",
    "df_parallel['A_squared'] = df_parallel['A'].parallel_apply(compute_square)\n",
    "df_parallel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optimizing File I/O (chunksize, compression)\n",
    "Reading and writing large files in chunks and with compression can optimize file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B         C         D         E\n",
      "0 -0.722498  0.538341 -0.095738  1.398888  0.062597\n",
      "              A         B         C         D         E\n",
      "10000  0.752074 -0.802763  1.104975 -1.570371 -0.212752\n",
      "              A         B         C         D         E\n",
      "20000 -0.676919  0.460245 -2.231921  0.553191  0.256725\n",
      "              A         B        C         D         E\n",
      "30000 -1.056872 -0.927621 -3.36556 -1.283336  0.587232\n",
      "              A         B         C         D         E\n",
      "40000  0.457738 -1.858737  1.298233  0.475711  0.624194\n",
      "             A        B         C         D         E\n",
      "50000 -1.20249  0.42562 -0.497545  0.651716 -3.196837\n",
      "              A       B         C         D         E\n",
      "60000 -0.627103 -0.3621  1.088399 -0.946004  0.825941\n",
      "              A         B         C         D        E\n",
      "70000  0.541732 -0.972174 -0.559216  1.085184 -0.98544\n",
      "              A         B         C         D         E\n",
      "80000  0.244615  0.029862  0.968331  0.536014 -0.202492\n",
      "             A         B         C         D         E\n",
      "90000  3.82962  0.067324  0.898569  1.196524 -1.891669\n"
     ]
    }
   ],
   "source": [
    "# Writing a large CSV file\n",
    "df_large = pd.DataFrame(np.random.randn(100000, 5), columns=list('ABCDE'))\n",
    "df_large.to_csv('data/local/large_file.csv', index=False)\n",
    "\n",
    "# Reading in chunks\n",
    "chunk_size = 10000\n",
    "chunks = pd.read_csv('data/local/large_file.csv', chunksize=chunk_size)\n",
    "\n",
    "# Processing each chunk\n",
    "for chunk in chunks:\n",
    "    print(chunk.head(1))  # Process each chunk as needed\n",
    "\n",
    "# Writing with compression\n",
    "df_large.to_csv('data/local/large_file_compressed.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Working with Large Datasets in Pandas\n",
    "Strategies for handling large datasets include using `chunksize`, `category` dtype, and filtering data early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.722498</td>\n",
       "      <td>0.538341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.770041</td>\n",
       "      <td>-0.102221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.999852</td>\n",
       "      <td>1.081321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.112298</td>\n",
       "      <td>-0.921257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.845955</td>\n",
       "      <td>0.351747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B\n",
       "0 -0.722498  0.538341\n",
       "1  0.770041 -0.102221\n",
       "2 -1.999852  1.081321\n",
       "3 -0.112298 -0.921257\n",
       "4 -0.845955  0.351747"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Efficient data filtering before loading entire DataFrame\n",
    "df_filtered = pd.read_csv('data/local/large_file.csv', usecols=['A', 'B'], nrows=50000)\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Using dask for Scalable DataFrames\n",
    "`dask` is a parallel computing library that can handle larger-than-memory DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.722498</td>\n",
       "      <td>0.538341</td>\n",
       "      <td>-0.095738</td>\n",
       "      <td>1.398888</td>\n",
       "      <td>0.062597</td>\n",
       "      <td>-0.184157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.770041</td>\n",
       "      <td>-0.102221</td>\n",
       "      <td>1.012833</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>-0.727072</td>\n",
       "      <td>0.667820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.999852</td>\n",
       "      <td>1.081321</td>\n",
       "      <td>0.544163</td>\n",
       "      <td>-0.058429</td>\n",
       "      <td>-0.349761</td>\n",
       "      <td>-0.918530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.112298</td>\n",
       "      <td>-0.921257</td>\n",
       "      <td>-1.053475</td>\n",
       "      <td>-0.882685</td>\n",
       "      <td>-1.336251</td>\n",
       "      <td>-1.033556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.845955</td>\n",
       "      <td>0.351747</td>\n",
       "      <td>-0.728274</td>\n",
       "      <td>-0.562573</td>\n",
       "      <td>-1.311634</td>\n",
       "      <td>-0.494207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C         D         E       sum\n",
       "0 -0.722498  0.538341 -0.095738  1.398888  0.062597 -0.184157\n",
       "1  0.770041 -0.102221  1.012833  0.002186 -0.727072  0.667820\n",
       "2 -1.999852  1.081321  0.544163 -0.058429 -0.349761 -0.918530\n",
       "3 -0.112298 -0.921257 -1.053475 -0.882685 -1.336251 -1.033556\n",
       "4 -0.845955  0.351747 -0.728274 -0.562573 -1.311634 -0.494207"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Converting a Pandas DataFrame to a Dask DataFrame\n",
    "ddf = dd.from_pandas(df_large, npartitions=10)\n",
    "\n",
    "# Performing operations in parallel with Dask\n",
    "ddf['sum'] = ddf['A'] + ddf['B']\n",
    "ddf.compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Speeding Up with NumPy Operations\n",
    "Using NumPy arrays directly for numerical computations can be faster than Pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas execution time: 0.011649608612060547 seconds\n",
      "NumPy execution time: 0.01202249526977539 seconds\n"
     ]
    }
   ],
   "source": [
    "# Using NumPy for mathematical operations\n",
    "values = np.random.rand(1000000)\n",
    "\n",
    "# Calculating square roots using Pandas\n",
    "start_time = time.time()\n",
    "df_values = pd.DataFrame(values, columns=['Values'])\n",
    "df_values['sqrt'] = df_values['Values'] ** 0.5\n",
    "end_time = time.time()\n",
    "print(f\"Pandas execution time: {end_time - start_time} seconds\")\n",
    "\n",
    "# Calculating square roots using NumPy\n",
    "start_time = time.time()\n",
    "sqrt_values = np.sqrt(values)\n",
    "end_time = time.time()\n",
    "print(f\"NumPy execution time: {end_time - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
